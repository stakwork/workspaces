import json
import base64
import time
import os
import json
import uuid
import yaml
import string
import random
import logging
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
from kubernetes import client, config
from datetime import datetime
import jwt
import bcrypt
from datetime import datetime, timedelta, timezone
from functools import wraps

# Add these constants at the top with your other configuration
JWT_SECRET_KEY = None
USERS_CONFIG = None

# Load JWT secret and users on startup
def load_auth_config():
    global JWT_SECRET_KEY, USERS_CONFIG
    
    try:
        # Get JWT secret from Kubernetes secret
        secret = core_v1.read_namespaced_secret("workspace-auth-secret", "workspace-system")
        JWT_SECRET_KEY = base64.b64decode(secret.data.get("jwt-secret")).decode('utf-8')
        logger.info("Loaded JWT secret from Kubernetes")
    except Exception as e:
        logger.error(f"Error loading JWT secret: {e}")
        JWT_SECRET_KEY = "fallback-secret-key-change-this"
    
    try:
        # Get users from ConfigMap
        config_map = core_v1.read_namespaced_config_map("workspace-users", "workspace-system")
        USERS_CONFIG = json.loads(config_map.data.get("users.json", '{"users": []}'))
        logger.info(f"Loaded {len(USERS_CONFIG.get('users', []))} users")
    except Exception as e:
        logger.error(f"Error loading users config: {e}")

app = Flask(__name__)
CORS(app)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    # Load in-cluster config
    config.load_incluster_config()
    logger.info("Loaded in-cluster Kubernetes configuration")
except config.config_exception.ConfigException:
    # Load kubeconfig for local development
    config.load_kube_config()
    logger.info("Loaded kubeconfig for local development")

# Initialize Kubernetes clients
core_v1 = client.CoreV1Api()
apps_v1 = client.AppsV1Api()
networking_v1 = client.NetworkingV1Api()

# Get domain from config map
try:
    config_map = core_v1.read_namespaced_config_map("workspace-config", "workspace-system")
    DOMAIN = config_map.data.get("domain", "SUBDOMAIN_REPLACE_ME")
    PARENT_DOMAIN = config_map.data.get("parent-domain", "REPLACE_ME")
    WORKSPACE_DOMAIN = config_map.data.get("workspace-domain", "SUBDOMAIN_REPLACE_ME")
    AWS_ACCOUNT_ID = config_map.data.get("aws-account-id", "AWS_ACCOUNT_ID")
    logger.info(f"Using domain: {DOMAIN}, parent domain: {PARENT_DOMAIN}, workspace domain: {WORKSPACE_DOMAIN}")

    load_auth_config()
except Exception as e:
    logger.error(f"Error reading config map: {e}")
    DOMAIN = "SUBDOMAIN_REPLACE_ME"
    PARENT_DOMAIN = "REPLACE_ME"
    WORKSPACE_DOMAIN = "SUBDOMAIN_REPLACE_ME"
    AWS_ACCOUNT_ID = "AWS_ACCOUNT_ID_REPLACE_ME"

def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None
        
        # Check for token in Authorization header
        if 'Authorization' in request.headers:
            auth_header = request.headers['Authorization']
            try:
                token = auth_header.split(" ")[1]  # Bearer <token>
            except IndexError:
                return jsonify({'error': 'Invalid token format'}), 401
        
        if not token:
            return jsonify({'error': 'Token is missing'}), 401
        
        try:
            # Decode the token
            data = jwt.decode(token, JWT_SECRET_KEY, algorithms=['HS256'])
            current_user = {
                'username': data['username'],
                'role': data.get('role', 'user'),
                'exp': data['exp']
            }
        except jwt.ExpiredSignatureError:
            return jsonify({'error': 'Token has expired'}), 401
        except jwt.InvalidTokenError:
            return jsonify({'error': 'Token is invalid'}), 401
        
        return f(current_user, *args, **kwargs)
    
    return decorated

def generate_random_subdomain(length=8):
    """Generate a random subdomain name"""
    letters = string.ascii_lowercase + string.digits
    return ''.join(random.choice(letters) for i in range(length))

def random_password(length=12):
    """Generate a random password"""
    chars = string.ascii_letters + string.digits
    return ''.join(random.choice(chars) for i in range(length))

@app.route('/api/workspaces', methods=['GET'])
@token_required
def list_workspaces(current_user):
    """List all workspaces"""
    workspaces = []
    
    try:
        # Get all namespaces with the workspace label
        namespaces = core_v1.list_namespace(label_selector="app=workspace")
        
        for ns in namespaces.items:
            try:
                # Get workspace info from config map
                config_maps = core_v1.list_namespaced_config_map(ns.metadata.name, label_selector="app=workspace-info")
                if not config_maps.items:
                    continue
                    
                workspace_info = json.loads(config_maps.items[0].data.get("info", "{}"))
                
                # Don't expose password
                if "password" in workspace_info:
                    workspace_info["password"] = "********"
                    
                # Get pods to determine state
                pods = core_v1.list_namespaced_pod(ns.metadata.name, label_selector="app=code-server")
                if pods.items:
                    if pods.items[0].status.phase == "Running":
                        workspace_info["state"] = "running"
                    else:
                        workspace_info["state"] = pods.items[0].status.phase.lower()
                else:
                    workspace_info["state"] = "unknown"
                    
                workspaces.append(workspace_info)
            except Exception as e:
                logger.error(f"Error getting workspace info from namespace {ns.metadata.name}: {e}")
                continue
    except Exception as e:
        logger.error(f"Error listing workspaces: {e}")
        return jsonify({"error": str(e)}), 500
        
    return jsonify({"workspaces": workspaces})

@app.route('/api/workspaces', methods=['POST'])
@token_required
def create_workspace(current_user):
    """Create a new workspace"""
    # Extract and validate request data
    workspace_config = _extract_workspace_config(request.json)
    
    # Generate workspace identifiers
    workspace_ids = _generate_workspace_identifiers()
    
    try:
        # Create the namespace
        _create_namespace(workspace_ids)
        
        # Create storage and credentials
        _create_persistent_volume_claim(workspace_ids)
        _create_workspace_secret(workspace_ids, workspace_config.get('github_token'))
        
        # Create initialization scripts
        _create_init_script_configmap(workspace_ids, workspace_config)
        _create_workspace_info_configmap(workspace_ids, workspace_config)

        # Copy required ConfigMaps and Secrets
        _copy_port_detector_configmap(workspace_ids)
        _copy_wildcard_certificate(workspace_ids)
        
        # Create Kubernetes resources
        _create_deployment(workspace_ids, workspace_config)
        _create_service(workspace_ids)
        _create_ingress(workspace_ids)
        
        return jsonify({
            "success": True,
            "message": "Workspace creation initiated",
            "workspace": _get_workspace_info(workspace_ids, workspace_config)
        })
        
    except Exception as e:
        logger.error(f"Error creating workspace: {e}")
        # Try to clean up if something went wrong
        try:
            core_v1.delete_namespace(workspace_ids['namespace_name'])
        except:
            pass
        return jsonify({"error": str(e)}), 500

def _create_post_start_command():
    """Create the post-start command for Docker setup with explicit Debian/Ubuntu handling"""
    return [
        "/bin/bash",
        "-c", 
        """
            exec > /workspaces/poststart.log 2>&1
            echo "Starting post-start initialization at $(date)"

            # Detect OS distribution
            if [ -f /etc/os-release ]; then
                . /etc/os-release
                OS=$ID
                VERSION_CODENAME=$VERSION_CODENAME
                echo "Detected OS: $OS $VERSION_CODENAME"
            else
                echo "Cannot detect OS, assuming Ubuntu"
                OS="ubuntu"
                VERSION_CODENAME="focal"
            fi

            # Install common dependencies
            apt-get update -y || {
                echo "WARNING: apt-get update failed, retrying with a delay"
                sleep 5
                apt-get update -y || echo "WARNING: apt-get update failed again, proceeding anyway"
            }
            apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release git

            # Wait for code-server to be ready before installing extensions
            echo "Waiting for code-server to be ready..."
            timeout=60
            while ! pgrep -f "code-server" > /dev/null; do
                echo "Waiting for code-server process to start..."
                sleep 2
                timeout=$((timeout - 1))
                if [ $timeout -le 0 ]; then
                    echo "Timeout waiting for code-server"
                    break
                fi
            done

            # Additional wait to ensure code-server is fully ready
            sleep 10

            # Check and install any extensions from devcontainer.json if not already installed
            if [ -f /workspaces/install-extensions.sh ] && [ -f /workspaces/.extensions-list ]; then
                echo "Running extension installation script again to ensure all extensions are installed"
                /workspaces/install-extensions.sh
            fi

            echo "Installing Docker for $OS $VERSION_CODENAME"
            
            # Function to check if Docker is running
            docker_running() {
                docker info &>/dev/null
            }
            
            # Function to check if Docker CLI is installed
            docker_installed() {
                command -v docker &>/dev/null
            }
            
            # Function to start Docker daemon
            start_docker_daemon() {
                echo "Starting Docker daemon"
                mkdir -p /var/run/docker
                chown root:docker /var/run/docker
                chmod 770 /var/run/docker
                
                # Check if dockerd is already running
                if pgrep dockerd; then
                    echo "Docker daemon is already running"
                    return 0
                fi
                
                # Start Docker daemon
                dockerd \
                    --host=unix:///var/run/docker.sock \
                    --host=tcp://127.0.0.1:2376 \
                    --storage-driver=overlay2 \
                    --tls=false &
                DOCKER_PID=$!
                echo "Docker daemon started with PID: $DOCKER_PID"
                
                # Wait for Docker to start
                timeout=30
                while ! docker_running; do
                    echo "Waiting for docker to start..."
                    if [ $timeout -le 0 ]; then
                        echo "Docker daemon failed to start"
                        return 1
                    fi
                    timeout=$((timeout - 1))
                    sleep 1
                done
                
                # Set proper permissions on Docker socket
                echo "Setting Docker socket permissions"
                chown root:docker /var/run/docker.sock
                chmod 666 /var/run/docker.sock
                
                return 0
            }
            
            # Install Docker based on distribution
            if [ "$OS" = "debian" ]; then
                # Debian-specific Docker installation
                echo "Setting up Docker for Debian $VERSION_CODENAME"
                
                # Install Docker using Debian approach
                install -m 0755 -d /etc/apt/keyrings
                curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
                chmod a+r /etc/apt/keyrings/docker.gpg
                
                echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian $VERSION_CODENAME stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
                
                apt-get update -y
                # Try to install Docker CE packages, with fallback
                apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin || {
                    echo "Standard Docker packages failed to install for Debian, trying docker.io"
                    apt-get install -y docker.io
                }
            elif [ "$OS" = "ubuntu" ]; then
                # Ubuntu-specific Docker installation
                echo "Setting up Docker for Ubuntu $VERSION_CODENAME"
                
                curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
                echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $VERSION_CODENAME stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
                
                apt-get update -y
                apt-get install -y docker-ce docker-ce-cli containerd.io || {
                    echo "Standard Docker packages failed to install for Ubuntu, trying docker.io"
                    apt-get install -y docker.io
                }
            else
                # Fallback for other distributions
                echo "Unknown distribution: $OS, attempting generic Docker installation"
                apt-get install -y docker.io || {
                    echo "Could not install docker.io, trying snap"
                    apt-get install -y snapd
                    snap install docker
                }
            fi

            echo "Docker installed. Checking version:"
            docker --version || echo "Docker command not found"
            
            # Setup Docker user and permissions
            echo "Setting up Docker group and permissions"
            groupadd -f docker
            getent passwd root > /dev/null && usermod -aG docker root
            getent passwd abc > /dev/null && usermod -aG docker abc
            getent passwd coder > /dev/null && usermod -aG docker coder
            getent passwd vscode > /dev/null && usermod -aG docker vscode
            
            # Start the Docker daemon if not already running
            if ! docker_running; then
                start_docker_daemon
            fi
            
            # Verify Docker is working
            if docker_running; then
                echo "Docker daemon started successfully"
                # Pull a few common images to speed up future operations
                echo "Pulling common Docker images in background"
                docker pull hello-world &>/dev/null &
                docker pull node:lts-slim &>/dev/null &
                docker pull python:3-slim &>/dev/null &
            else
                echo "WARNING: Docker daemon is not running properly"
                echo "Trying to fix Docker setup..."
                
                # Try to fix Docker setup
                pkill dockerd
                sleep 2
                rm -f /var/run/docker.pid
                rm -f /var/run/docker.sock
                
                start_docker_daemon
                
                if docker_running; then
                    echo "Docker fixed and is now running"
                else
                    echo "WARNING: Could not fix Docker, it may not be available"
                fi
            fi
            
            # Install Docker Compose if not already installed
            if ! command -v docker-compose &>/dev/null; then
                echo "Installing Docker Compose"
                mkdir -p /usr/local/lib/docker/cli-plugins
                COMPOSE_VERSION="v2.24.6"
                curl -SL "https://github.com/docker/compose/releases/download/${COMPOSE_VERSION}/docker-compose-linux-$(uname -m)" -o /usr/local/bin/docker-compose
                chmod +x /usr/local/bin/docker-compose
                ln -sf /usr/local/bin/docker-compose /usr/local/lib/docker/cli-plugins/docker-compose
                echo "Docker Compose installed:"
                docker-compose --version || echo "Docker Compose installation failed"
            fi
            
            # Execute feature installation if the script exists
            if [ -f /workspaces/install-features.sh ]; then
                echo "Running feature installation script"
                /workspaces/install-features.sh
            else
                echo "No feature installation script found"
            fi

            # Run start-docker-compose command if it exists
            if [ -f "/workspaces/start-docker-compose.sh" ]; then
                echo "Running docker-compose..."
                /workspaces/start-docker-compose.sh
            fi

            cd /workspaces

            # Run post-create command if it exists
            if [ -f "/workspaces/post-create-command.sh" ]; then
                echo "Running postCreateCommand..."
                /workspaces/post-create-command.sh
            fi

            
            echo "Post-start initialization completed at $(date)"
        """
    ]

def _create_port_detector_container():
    """Create the port detector container"""
    return client.V1Container(
        name="port-detector",
        image="ubuntu:22.04",
        command=["/bin/bash", "/scripts/port-detector.sh"],
        security_context=client.V1SecurityContext(
            run_as_user=0  # Run as root to install packages
        ),
        volume_mounts=[
            client.V1VolumeMount(
                name="port-detector-script",
                mount_path="/scripts"
            )
        ]
    )


def _create_service(workspace_ids):
    """Create service for the code-server"""
    service = client.V1Service(
        metadata=client.V1ObjectMeta(
            name="code-server",
            namespace=workspace_ids['namespace_name'],
            labels={"app": "workspace"}
        ),
        spec=client.V1ServiceSpec(
            selector={"app": "code-server"},
            ports=[
                client.V1ServicePort(
                    name="code-server-port",
                    port=8444,
                    target_port=8444
                )
            ]
        )
    )
    core_v1.create_namespaced_service(workspace_ids['namespace_name'], service)
    logger.info(f"Created service in namespace: {workspace_ids['namespace_name']}")


def _create_ingress(workspace_ids):
    """Create ingress for the code-server"""
    ingress = client.V1Ingress(
        metadata=client.V1ObjectMeta(
            name="code-server",
            namespace=workspace_ids['namespace_name'],
            labels={"app": "workspace"},
            annotations={
                "kubernetes.io/ingress.class": "nginx",
                "nginx.ingress.kubernetes.io/proxy-read-timeout": "3600",
                "nginx.ingress.kubernetes.io/proxy-send-timeout": "3600"
            }
        ),
        spec=client.V1IngressSpec(
            tls=[
                client.V1IngressTLS(
                    hosts=[workspace_ids['fqdn']],
                    secret_name="workspace-domain-wildcard-tls"
                )
            ],
            rules=[
                client.V1IngressRule(
                    host=workspace_ids['fqdn'],
                    http=client.V1HTTPIngressRuleValue(
                        paths=[
                            client.V1HTTPIngressPath(
                                path="/",
                                path_type="Prefix",
                                backend=client.V1IngressBackend(
                                    service=client.V1IngressServiceBackend(
                                        name="code-server",
                                        port=client.V1ServiceBackendPort(
                                            number=8444
                                        )
                                    )
                                )
                            )
                        ]
                    )
                )
            ]
        )
    )
    networking_v1.create_namespaced_ingress(workspace_ids['namespace_name'], ingress)
    logger.info(f"Created ingress in namespace: {workspace_ids['namespace_name']}")

def _extract_workspace_config(data):
    """Extract and validate workspace configuration from request data"""
    github_urls = data.get('githubUrls', [])
    github_branches = data.get('githubBranches', [])

    if data.get('githubUrl') and not github_urls:
        github_urls = [data.get('githubUrl')]
        # Handle single branch for backward compatibility
        if data.get('githubBranch'):
            github_branches = [data.get('githubBranch')]
    
    if not github_urls:
        raise ValueError("At least one GitHub URL is required")
    
    # Ensure branches array matches URLs array length
    while len(github_branches) < len(github_urls):
        github_branches.append("")  # Empty string for default branch

    # Extract primary repo details
    primary_repo_url = github_urls[0].rstrip('/')
    repo_parts = primary_repo_url.split('/')
    repo_name = repo_parts[-1].replace('.git', '') if len(repo_parts) > 1 else "unknown"
    
    # Get custom image configuration
    custom_image = data.get('image', 'linuxserver/code-server:latest')
    custom_image_url = data.get('imageUrl', '')
    use_custom_image_url = bool(custom_image_url)
    use_dev_container = data.get('useDevContainer', True)

    # Get optional GitHub token
    github_token = data.get('githubToken', None)
        
    if use_custom_image_url:
        logger.info(f"Custom image URL provided: {custom_image_url}")
    else:
        logger.info(f"Using specified Docker image: {custom_image}")
        if use_dev_container:
            logger.info(f"Using dev container mode with image: {custom_image}")
    
    return {
        'github_urls': github_urls,
        'github_branches': github_branches,
        'primary_repo_url': primary_repo_url,
        'repo_name': repo_name,
        'custom_image': custom_image,
        'custom_image_url': custom_image_url,
        'use_custom_image_url': use_custom_image_url,
        'use_dev_container': use_dev_container,
        'github_token': github_token
    }


def _generate_workspace_identifiers():
    """Generate unique identifiers for the workspace"""
    build_timestamp = int(time.time())
    workspace_id = str(uuid.uuid4())[:8]
    subdomain = generate_random_subdomain()
    namespace_name = f"workspace-{workspace_id}"
    fqdn = f"{subdomain}.{WORKSPACE_DOMAIN}"
    password = random_password()
    
    return {
        'workspace_id': workspace_id,
        'subdomain': subdomain,
        'namespace_name': namespace_name,
        'fqdn': fqdn,
        'build_timestamp': build_timestamp,
        'password': password
    }


def _create_namespace(workspace_ids):
    """Create the Kubernetes namespace for the workspace"""
    namespace = client.V1Namespace(
        metadata=client.V1ObjectMeta(
            name=workspace_ids['namespace_name'],
            labels={
                "app": "workspace",
                "workspaceId": workspace_ids['workspace_id'],
                "allowed-registry-access": "true"
            }
        )
    )
    core_v1.create_namespace(namespace)
    logger.info(f"Created namespace: {workspace_ids['namespace_name']}")


def _create_persistent_volume_claim(workspace_ids):
    """Create PVC for workspace data"""
    pvc = client.V1PersistentVolumeClaim(
        metadata=client.V1ObjectMeta(
            name="workspace-data",
            namespace=workspace_ids['namespace_name'],
            labels={"app": "workspace"}
        ),
        spec=client.V1PersistentVolumeClaimSpec(
            access_modes=["ReadWriteMany"],
            resources=client.V1ResourceRequirements(
                requests={"storage": "10Gi"}
            ),
            storage_class_name="efs-sc"
        )
    )
    core_v1.create_namespaced_persistent_volume_claim(workspace_ids['namespace_name'], pvc)
    logger.info(f"Created PVC in namespace: {workspace_ids['namespace_name']}")


def _create_workspace_secret(workspace_ids, github_token=None):
    """Create secret for workspace credentials and optional GitHub token"""
    string_data = {
        "password": workspace_ids['password']
    }

    if github_token:
        string_data["github_token"] = github_token

    secret = client.V1Secret(
        metadata=client.V1ObjectMeta(
            name="workspace-secret",
            namespace=workspace_ids['namespace_name'],
            labels={"app": "workspace"}
        ),
        string_data=string_data
    )
    core_v1.create_namespaced_secret(workspace_ids['namespace_name'], secret)
    logger.info(f"Created secret in namespace: {workspace_ids['namespace_name']}")

def _create_feature_installation_script():
    """Generate a script that handles common dev container features installation"""
    return """#!/bin/bash
# Script to install common dev container features
set -e

FEATURES_FILE="/workspaces/.devcontainer-features"
if [ ! -f "$FEATURES_FILE" ]; then
    echo "No features file found, skipping feature installation"
    exit 0
fi

echo "Installing dev container features from features file"
echo "Features content:"
cat "$FEATURES_FILE"

# Convert features file to JSON for easier parsing
FEATURES_JSON=$(cat "$FEATURES_FILE")

# Helper function to check if a feature exists
feature_exists() {
    echo "$FEATURES_JSON" | grep -q "\"$1\""
}

# Helper to extract feature version/options
get_feature_option() {
    local feature=$1
    local option=$2
    local default=$3
    
    # Try to extract the version or option using grep and sed
    # Format is typically "feature": { "version": "value", "optionName": "value" }
    result=$(echo "$FEATURES_JSON" | grep -o "\"$feature\"[^}]*" | grep -o "\"$option\"[^,}]*" | grep -o "\"[^\"]*\"$" | tr -d '"' || echo "")
    
    if [ -z "$result" ]; then
        echo "$default"
    else
        echo "$result"
    fi
}

# Install Docker feature
if feature_exists "docker"; then
    echo "Installing Docker feature"
    # Docker is already installed by the post-start script
    echo "✓ Docker already configured"
fi

# Install Docker-in-Docker feature (alternative to Docker)
if feature_exists "docker-in-docker" || feature_exists "docker-from-docker"; then
    echo "Installing Docker-in-Docker feature"
    # Docker is already installed by the post-start script
    echo "✓ Docker already configured via post-start script"
    
    # Add Docker Compose v2 if not already added
    if ! command -v docker-compose &> /dev/null; then
        echo "Installing Docker Compose v2"
        mkdir -p /usr/local/lib/docker/cli-plugins
        curl -SL "https://github.com/docker/compose/releases/download/v2.24.6/docker-compose-linux-$(uname -m)" -o /usr/local/lib/docker/cli-plugins/docker-compose
        chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
        ln -sf /usr/local/lib/docker/cli-plugins/docker-compose /usr/local/bin/docker-compose
        echo "✓ Docker Compose installed: $(docker-compose version)"
    fi
fi

# Install Node.js feature
if feature_exists "node"; then
    echo "Installing Node.js feature"
    VERSION=$(get_feature_option "node" "version" "lts")
    
    # Install Node.js using NVM
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.3/install.sh | bash
    export NVM_DIR="$HOME/.nvm"
    [ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"
    
    if [ "$VERSION" = "lts" ] || [ "$VERSION" = "latest" ]; then
        nvm install --lts
    else
        nvm install "$VERSION"
    fi
    
    # Add NVM to shell initialization
    echo 'export NVM_DIR="$HOME/.nvm"' >> /etc/profile.d/nvm.sh
    echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> /etc/profile.d/nvm.sh
    
    echo "✓ Node.js $(node -v) installed"
fi

# Install Python feature
if feature_exists "python"; then
    echo "Installing Python feature"
    VERSION=$(get_feature_option "python" "version" "3.10")
    INSTALL_TOOLS=$(get_feature_option "python" "installTools" "true")
    INSTALL_JUPYTER=$(get_feature_option "python" "installJupyter" "false")
    
    # Install Python with apt
    apt-get update
    apt-get install -y python3 python3-pip python3-venv
    
    # Create symbolic links
    ln -sf /usr/bin/python3 /usr/bin/python
    
    echo "✓ Python $(python3 --version) installed"
    
    # Install common tools if requested
    if [ "$INSTALL_TOOLS" = "true" ]; then
        echo "Installing Python tools"
        pip3 install --no-cache-dir ipython pytest pylint flake8 black
        echo "✓ Common Python tools installed"
    fi
    
    # Install Jupyter if requested
    if [ "$INSTALL_JUPYTER" = "true" ]; then
        echo "Installing Jupyter"
        pip3 install --no-cache-dir jupyter notebook
        echo "✓ Jupyter installed"
    fi
fi

# Install Go feature
if feature_exists "go"; then
    echo "Installing Go feature"
    VERSION=$(get_feature_option "go" "version" "latest")
    
    if [ "$VERSION" = "latest" ]; then
        VERSION=$(curl -s https://go.dev/VERSION?m=text | head -n1)
    fi
    
    # Download and install Go
    curl -sSL "https://golang.org/dl/${VERSION}.linux-amd64.tar.gz" -o go.tar.gz
    tar -C /usr/local -xzf go.tar.gz
    rm go.tar.gz
    
    # Add Go to PATH
    echo 'export PATH=$PATH:/usr/local/go/bin' > /etc/profile.d/go.sh
    echo 'export PATH=$PATH:$HOME/go/bin' >> /etc/profile.d/go.sh
    
    # Set up environment for current session
    export PATH=$PATH:/usr/local/go/bin
    
    echo "✓ Go $(go version) installed"
fi

# Install Java feature
if feature_exists "java"; then
    echo "Installing Java feature"
    VERSION=$(get_feature_option "java" "version" "17")
    
    # Install OpenJDK
    apt-get update
    apt-get install -y openjdk-${VERSION}-jdk
    
    echo "✓ Java $(java -version 2>&1 | head -n 1) installed"
fi

# Install Rust feature
if feature_exists "rust"; then
    echo "Installing Rust feature"
    
    # Install Rust using rustup
    curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
    
    # Add Rust to PATH
    echo 'export PATH=$PATH:$HOME/.cargo/bin' > /etc/profile.d/rust.sh
    
    # Set up environment for current session
    export PATH=$PATH:$HOME/.cargo/bin
    
    echo "✓ Rust $(rustc --version) installed"
fi

# Install .NET feature
if feature_exists "dotnet"; then
    echo "Installing .NET feature"
    VERSION=$(get_feature_option "dotnet" "version" "latest")
    
    # Install .NET SDK
    apt-get update
    apt-get install -y wget
    
    if [ "$VERSION" = "latest" ]; then
        wget https://dot.net/v1/dotnet-install.sh -O dotnet-install.sh
        chmod +x dotnet-install.sh
        ./dotnet-install.sh
    else
        wget https://dot.net/v1/dotnet-install.sh -O dotnet-install.sh
        chmod +x dotnet-install.sh
        ./dotnet-install.sh --version $VERSION
    fi
    
    # Add .NET to PATH
    echo 'export PATH=$PATH:$HOME/.dotnet' > /etc/profile.d/dotnet.sh
    
    # Set up environment for current session
    export PATH=$PATH:$HOME/.dotnet
    
    echo "✓ .NET installed"
fi

# Install PHP feature
if feature_exists "php"; then
    echo "Installing PHP feature"
    VERSION=$(get_feature_option "php" "version" "8.2")
    COMPOSER=$(get_feature_option "php" "composer" "true")
    
    # Install PHP
    apt-get update
    apt-get install -y software-properties-common
    add-apt-repository -y ppa:ondrej/php
    apt-get update
    apt-get install -y php${VERSION} php${VERSION}-cli php${VERSION}-common php${VERSION}-curl php${VERSION}-mbstring php${VERSION}-mysql php${VERSION}-xml php${VERSION}-zip
    
    # Install Composer if requested
    if [ "$COMPOSER" = "true" ]; then
        echo "Installing Composer"
        curl -sS https://getcomposer.org/installer | php -- --install-dir=/usr/local/bin --filename=composer
        echo "✓ Composer installed: $(composer --version)"
    fi
    
    echo "✓ PHP installed: $(php -v | head -n 1)"
fi

# Install common utilities
if feature_exists "common-utils"; then
    echo "Installing common utilities"
    
    apt-get update
    apt-get install -y wget curl vim git jq unzip zip sudo 
    apt-get install -y build-essential pkg-config libssl-dev
    
    echo "✓ Common utilities installed"
fi

# Install GitHub CLI
if feature_exists "github-cli"; then
    echo "Installing GitHub CLI"
    
    # Install GitHub CLI
    curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
    chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null
    apt-get update
    apt-get install -y gh
    
    echo "✓ GitHub CLI $(gh --version | head -n 1) installed"
fi

# Install Azure CLI
if feature_exists "azure-cli"; then
    echo "Installing Azure CLI"
    
    curl -sL https://aka.ms/InstallAzureCLIDeb | bash
    
    echo "✓ Azure CLI $(az --version | head -n 1) installed"
fi

# Install AWS CLI
if feature_exists "aws-cli"; then
    echo "Installing AWS CLI"
    
    apt-get update
    apt-get install -y unzip
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    unzip awscliv2.zip
    ./aws/install
    rm -rf aws awscliv2.zip
    
    echo "✓ AWS CLI $(aws --version) installed"
fi

# Install Terraform
if feature_exists "terraform"; then
    echo "Installing Terraform"
    VERSION=$(get_feature_option "terraform" "version" "latest")
    
    apt-get update
    apt-get install -y gnupg software-properties-common curl
    
    curl -fsSL https://apt.releases.hashicorp.com/gpg | apt-key add -
    apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
    apt-get update
    
    if [ "$VERSION" = "latest" ]; then
        apt-get install -y terraform
    else
        apt-get install -y terraform=$VERSION
    fi
    
    echo "✓ Terraform $(terraform version | head -n 1) installed"
fi

# Install kubectl
if feature_exists "kubectl" || feature_exists "kubernetes-tools"; then
    echo "Installing kubectl"
    VERSION=$(get_feature_option "kubectl" "version" "latest")
    
    if [ "$VERSION" = "latest" ]; then
        VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
    fi
    
    curl -LO "https://dl.k8s.io/release/$VERSION/bin/linux/amd64/kubectl"
    chmod +x kubectl
    mv kubectl /usr/local/bin/
    
    echo "✓ kubectl $(kubectl version --client -o json | jq -r '.clientVersion.gitVersion') installed"
fi

echo "Feature installation completed"
"""

def _create_init_script_configmap(workspace_ids, workspace_config):
    """Create ConfigMap with initialization scripts"""
    # Start with base repository cloning script
    init_script = _generate_init_script(workspace_ids, workspace_config)
    
    namespace_name = workspace_ids['namespace_name']
    repo_name = workspace_config['repo_name']

    # Add code to create a wrapper Dockerfile that uses the user's Dockerfile as a base
    init_script += f"""
    # Create directory for wrapper Dockerfile and user Dockerfile
    mkdir -p /workspaces/.code-server-wrapper
    mkdir -p /workspaces/.user-dockerfile
    cd /workspaces/.code-server-wrapper
    
    # Locate the user's Dockerfile in their repo
    USER_REPO_PATH="/workspaces/{repo_name}"
    DOCKERFILE_PATH="$USER_REPO_PATH/.devcontainer/Dockerfile"
    DEVCONTAINER_JSON_PATH="$USER_REPO_PATH/.devcontainer/devcontainer.json"
    
    """
    
    # Continuing the script
    init_script += """
    # Debug info
    echo "DEBUG: Checking repository and Dockerfile"
    if [ -d "$USER_REPO_PATH" ]; then
        echo "DEBUG: Repository directory exists at $USER_REPO_PATH"
        ls -la "$USER_REPO_PATH"
    else
        echo "DEBUG: ERROR - Repository directory does not exist at $USER_REPO_PATH"
    fi
    
    if [ -d "$USER_REPO_PATH/.devcontainer" ]; then
        echo "DEBUG: .devcontainer directory exists"
        ls -la "$USER_REPO_PATH/.devcontainer"
    else
        echo "DEBUG: .devcontainer directory does not exist"
    fi
    
    if [ -f "$DOCKERFILE_PATH" ]; then
        echo "DEBUG: Dockerfile exists at $DOCKERFILE_PATH"
        cat "$DOCKERFILE_PATH" | head -n 10
    else
        echo "DEBUG: Dockerfile does not exist at $DOCKERFILE_PATH"
    fi

    # Check for devcontainer.json
    if [ -f "$DEVCONTAINER_JSON_PATH" ]; then
        echo "DEBUG: devcontainer.json exists at $DEVCONTAINER_JSON_PATH"
        cat "$DEVCONTAINER_JSON_PATH" | head -n 20
    else
        echo "DEBUG: devcontainer.json does not exist at $DEVCONTAINER_JSON_PATH"
    fi
    """
    
    # Clone repository if needed
    init_script += f"""
    # Check if the first repository actually got cloned
    if [ ! -d "$USER_REPO_PATH" ]; then
        echo "Repository not found at $USER_REPO_PATH, attempting to clone again"
        cd /workspaces
        
        # Get the branch for the first repository
        BRANCH="{workspace_config['github_branches'][0] if workspace_config['github_branches'] and workspace_config['github_branches'][0] else ''}"
        
        if [ ! -z "$BRANCH" ]; then
            echo "Cloning with specific branch: $BRANCH"
            git clone -b $BRANCH {workspace_config['github_urls'][0]} {repo_name}
        else
            echo "Cloning with default branch"
            git clone {workspace_config['github_urls'][0]} {repo_name}
        fi

        git config --global --add safe.directory /workspaces/{repo_name}
    fi
    """
    
    # Main processing script
    init_script += f"""
    # Check again after potential re-cloning
    if [ -f "$DOCKERFILE_PATH" ]; then
        echo "Found user Dockerfile at $DOCKERFILE_PATH"
        # Copy the user's Dockerfile
        cp "$DOCKERFILE_PATH" /workspaces/.user-dockerfile/Dockerfile
        
        # Process devcontainer.json if it exists
        if [ -f "$DEVCONTAINER_JSON_PATH" ]; then
            echo "Found devcontainer.json - processing configuration"
            
            # Copy the devcontainer.json file to the build context
            cp "$DEVCONTAINER_JSON_PATH" /workspaces/.user-dockerfile/
            
            # Install jq if needed for JSON processing
            if ! command -v jq &> /dev/null; then
                echo "Installing jq to parse devcontainer.json"
                apt-get update && apt-get install -y jq
            fi
            
            # Extract extensions from devcontainer.json (support both formats)
            EXTENSIONS=$(jq -r '.extensions[]? // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ -z "$EXTENSIONS" ]; then
                EXTENSIONS=$(jq -r '.customizations.vscode.extensions[]? // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            fi
            
            # Save extensions to file if found
            if [ ! -z "$EXTENSIONS" ]; then
                echo "Found extensions in devcontainer.json:"
                echo "$EXTENSIONS"
                echo "$EXTENSIONS" > /workspaces/.extensions-list
            else
                echo "No extensions found in devcontainer.json or couldn't parse"
            fi
            
            # Extract VS Code settings
            SETTINGS=$(jq -r '.settings // .customizations.vscode.settings // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$SETTINGS" ]; then
                echo "Found VS Code settings in devcontainer.json"
                mkdir -p /workspaces/.vscode
                echo "$SETTINGS" > /workspaces/.vscode/settings.json
            fi
            
            # Extract features
            FEATURES=$(jq -r '.features // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$FEATURES" ]; then
                echo "Found features in devcontainer.json:"
                echo "$FEATURES" > /workspaces/.devcontainer-features
                echo "Features will be installed during workspace initialization"
            fi
            
            # Extract port forwarding configuration
            PORTS=$(jq -r '.forwardPorts[]? // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$PORTS" ]; then
                echo "Found ports to forward in devcontainer.json:"
                echo "$PORTS"
                echo "$PORTS" > /workspaces/.forward-ports
            fi
            
            # Extract other customizations
            CUSTOMIZATIONS=$(jq -r '.customizations // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$CUSTOMIZATIONS" ]; then
                echo "Found customizations in devcontainer.json"
                echo "$CUSTOMIZATIONS" > /workspaces/.customizations
            fi
            
            # Extract environment variables
            ENV_VARS=$(jq -r '.containerEnv // empty | to_entries[] | "\\(.key)=\\(.value)"' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$ENV_VARS" ]; then
                echo "Found environment variables in devcontainer.json:"
                echo "$ENV_VARS"
                echo "$ENV_VARS" > /workspaces/.container-env
            fi
            
            # Extract remote environment variables
            REMOTE_ENV_VARS=$(jq -r '.remoteEnv // empty | to_entries[] | "\\(.key)=\\(.value)"' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$REMOTE_ENV_VARS" ]; then
                echo "Found remote environment variables in devcontainer.json:"
                echo "$REMOTE_ENV_VARS"
                echo "$REMOTE_ENV_VARS" > /workspaces/.remote-env
            fi
            
            # Extract user configuration
            REMOTE_USER=$(jq -r '.remoteUser // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            CONTAINER_USER=$(jq -r '.containerUser // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            
            if [ ! -z "$REMOTE_USER" ] || [ ! -z "$CONTAINER_USER" ]; then
                echo "Found user configuration in devcontainer.json"
                
                if [ ! -z "$REMOTE_USER" ]; then
                    echo "remoteUser: $REMOTE_USER"
                    echo "REMOTE_USER=$REMOTE_USER" > /workspaces/.user-config
                fi
                
                if [ ! -z "$CONTAINER_USER" ]; then
                    echo "containerUser: $CONTAINER_USER"
                    echo "CONTAINER_USER=$CONTAINER_USER" >> /workspaces/.user-config
                fi
            fi
            
            # Extract lifecycle commands
            POST_CREATE_CMD=$(jq -r '.postCreateCommand // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$POST_CREATE_CMD" ]; then
                echo "Found postCreateCommand in devcontainer.json"
                echo "#!/bin/bash" > /workspaces/post-create-command.sh
                echo "$POST_CREATE_CMD" >> /workspaces/post-create-command.sh
                chmod +x /workspaces/post-create-command.sh
            fi
            
            POST_START_CMD=$(jq -r '.postStartCommand // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            if [ ! -z "$POST_START_CMD" ]; then
                echo "Found postStartCommand in devcontainer.json"
                echo "#!/bin/bash" > /workspaces/post-start-command.sh
                echo "$POST_START_CMD" >> /workspaces/post-start-command.sh
                chmod +x /workspaces/post-start-command.sh
            fi

            DOCKER_COMPOSE_FILE=$(jq -r '.dockerComposeFile // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            SERVICE_NAME=$(jq -r '.service // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)
            WORKSPACE_FOLDER=$(jq -r '.workspaceFolder // empty' "$DEVCONTAINER_JSON_PATH" 2>/dev/null)

            if [ ! -z "$DOCKER_COMPOSE_FILE" ]; then
                echo "Found dockerComposeFile in devcontainer.json: $DOCKER_COMPOSE_FILE"
                echo "Service: $SERVICE_NAME"
                echo "Workspace folder: $WORKSPACE_FOLDER"
                
                # Save docker-compose configuration
                echo "{repo_name}/.devcontainer/$DOCKER_COMPOSE_FILE" > /workspaces/.docker-compose-file
                [ ! -z "$SERVICE_NAME" ] && echo "$SERVICE_NAME" > /workspaces/.docker-compose-service
                [ ! -z "$WORKSPACE_FOLDER" ] && echo "$WORKSPACE_FOLDER" > /workspaces/.docker-compose-workspace-folder
                
                echo "Docker Compose configuration will be started during workspace initialization"
            fi

            """

    init_script += """

# Create docker-compose startup script
echo "Creating docker-compose startup script"
cat > /workspaces/start-docker-compose.sh << 'EOL'
#!/bin/bash
set -e

COMPOSE_FILE_PATH="/workspaces/.docker-compose-file"
SERVICE_FILE_PATH="/workspaces/.docker-compose-service"
WORKSPACE_FOLDER_FILE="/workspaces/.docker-compose-workspace-folder"

if [ ! -f "$COMPOSE_FILE_PATH" ]; then
    echo "No docker-compose configuration found"
    exit 0
fi

DOCKER_COMPOSE_FILE=$(cat "$COMPOSE_FILE_PATH")
SERVICE_NAME=""
WORKSPACE_FOLDER="/workspaces"

if [ -f "$SERVICE_FILE_PATH" ]; then
    SERVICE_NAME=$(cat "$SERVICE_FILE_PATH")
fi

if [ -f "$WORKSPACE_FOLDER_FILE" ]; then
    WORKSPACE_FOLDER=$(cat "$WORKSPACE_FOLDER_FILE")
fi

echo "Starting Docker Compose setup..."
echo "Compose file: $DOCKER_COMPOSE_FILE"
echo "Service: $SERVICE_NAME"
echo "Workspace folder: $WORKSPACE_FOLDER"

# Change to the directory containing the docker-compose file
cd /workspaces

# Find the docker-compose file (could be relative to .devcontainer or repo root)
COMPOSE_FILE_FULL_PATH=""

# Check in .devcontainer directory first
if [ -f ".devcontainer/$DOCKER_COMPOSE_FILE" ]; then
    COMPOSE_FILE_FULL_PATH=".devcontainer/$DOCKER_COMPOSE_FILE"
    cd /workspaces
elif [ -f "$DOCKER_COMPOSE_FILE" ]; then
    COMPOSE_FILE_FULL_PATH="$DOCKER_COMPOSE_FILE"
    cd /workspaces
else
    # Look for it in the first repository directory
    for repo_dir in */; do
        if [ -f "$repo_dir/.devcontainer/$DOCKER_COMPOSE_FILE" ]; then
            COMPOSE_FILE_FULL_PATH="$repo_dir/.devcontainer/$DOCKER_COMPOSE_FILE"
            cd "/workspaces/$repo_dir"
            break
        elif [ -f "$repo_dir/$DOCKER_COMPOSE_FILE" ]; then
            COMPOSE_FILE_FULL_PATH="$repo_dir/$DOCKER_COMPOSE_FILE"
            cd "/workspaces/$repo_dir"
            break
        fi
    done
fi

if [ -z "$COMPOSE_FILE_FULL_PATH" ]; then
    echo "ERROR: Could not find docker-compose file: $DOCKER_COMPOSE_FILE"
    exit 1
fi

echo "Found docker-compose file at: $COMPOSE_FILE_FULL_PATH"
echo "Working directory: $(pwd)"

# Ensure Docker is running
echo "Checking Docker daemon..."
timeout=30
while ! docker info >/dev/null 2>&1; do
    if [ $timeout -le 0 ]; then
        echo "ERROR: Docker daemon is not running"
        exit 1
    fi
    echo "Waiting for Docker daemon to start..."
    timeout=$((timeout - 1))
    sleep 1
done

echo "Docker daemon is ready"

# Start the docker-compose services
echo "Starting Docker Compose services..."

if [ ! -z "$SERVICE_NAME" ]; then
    echo "Starting specific service: $SERVICE_NAME"
    docker-compose -f "$COMPOSE_FILE_FULL_PATH" up -d "$SERVICE_NAME"
    
    # If this is a dev container setup, we might want to exec into the service
    echo "Docker Compose service '$SERVICE_NAME' is running"
    
    # Optional: Get the container ID for the service
    CONTAINER_ID=$(docker-compose -f "$COMPOSE_FILE_FULL_PATH" ps -q "$SERVICE_NAME")
    if [ ! -z "$CONTAINER_ID" ]; then
        echo "Service container ID: $CONTAINER_ID"
        echo "$CONTAINER_ID" > /workspaces/.service-container-id
        
        # You could potentially exec into this container or forward ports
        echo "Service is accessible via container: $CONTAINER_ID"
    fi
else
    echo "Starting all services"
    docker-compose -f "$COMPOSE_FILE_FULL_PATH" up -d
fi

# Show running containers
echo "Docker Compose services status:"
docker-compose -f "$COMPOSE_FILE_FULL_PATH" ps

# Save the compose file path for later use
echo "$COMPOSE_FILE_FULL_PATH" > /workspaces/.active-compose-file
echo "$(pwd)" > /workspaces/.compose-working-directory

echo "Docker Compose startup completed successfully"
EOL
chmod +x /workspaces/start-docker-compose.sh

# Create a comprehensive extension installation script
echo "Creating extension installation script"
cat > /workspaces/install-extensions.sh << 'EOL'
#!/bin/bash
EXTENSIONS_FILE="/workspaces/.extensions-list"

if [ -f "$EXTENSIONS_FILE" ]; then
  echo "Installing extensions from devcontainer.json..."

  # Set the correct extension directory for the web UI
  export VSCODE_EXTENSIONS="/config/extensions"
  export CODE_SERVER_EXTENSIONS_DIR="/config/extensions"
  
  # Ensure directories exist with proper permissions
  mkdir -p /config/extensions
  mkdir -p /config/data/User
  mkdir -p /config/data/logs
  
  # Create cache directory for extensions
  mkdir -p /workspaces/.vscode-extensions-cache
  
  # Function to install extension properly
  install_extension() {
    local extension="$1"
    echo "Installing extension: $extension"
    
    # Install using the web-based code-server with explicit paths
    if /usr/bin/code-server \
        --extensions-dir /config/extensions \
        --user-data-dir /config/data \
        --install-extension "$extension" 2>&1 | tee -a /workspaces/extension-install.log; then
      echo "Successfully installed: $extension"
      return 0
    else
      echo "Failed to install: $extension, trying alternative method..."
      
      # Alternative: try with different approach
      if code-server \
          --extensions-dir=/config/extensions \
          --user-data-dir=/config/data \
          --install-extension="$extension"; then
        echo "Successfully installed with alternative method: $extension"
        return 0
      else
        echo "Failed to install with all methods: $extension"
        return 1
      fi
    fi
  }
  
  # Install all extensions
  while IFS= read -r extension; do
    if [ ! -z "$extension" ]; then
      extension=$(echo "$extension" | tr -d '"' | tr -d "'" | xargs)
      install_extension "$extension"
      sleep 3  # Longer delay between installations
    fi
  done < "$EXTENSIONS_FILE"
  
  # Force refresh of extension cache
  rm -rf /config/data/CachedExtensions
  rm -rf /config/data/logs/extension-host*
  
  # List extensions from the correct directory
  echo "=== Extension Installation Report ==="
  echo "Extensions in /config/extensions:"
  ls -la /config/extensions/ || echo "No extensions directory found"
  echo "Extensions reported by code-server:"
  /usr/bin/code-server --extensions-dir /config/extensions --user-data-dir /config/data --list-extensions || echo "Could not list extensions"
  echo "========================================="
else
  echo "No extensions list found"
fi
EOL
            chmod +x /workspaces/install-extensions.sh

            # Create a script to apply environment variables
            echo "Creating environment setup script"
            cat > /workspaces/setup-env.sh << 'EOL'
#!/bin/bash
# Apply container environment variables
if [ -f "/workspaces/.container-env" ]; then
  echo "Applying container environment variables"
  while IFS= read -r env_var; do
    if [ ! -z "$env_var" ]; then
      export "$env_var"
      echo "Exported: $env_var"
    fi
  done < "/workspaces/.container-env"
fi

# Apply remote environment variables
if [ -f "/workspaces/.remote-env" ]; then
  echo "Applying remote environment variables"
  while IFS= read -r env_var; do
    if [ ! -z "$env_var" ]; then
      export "$env_var"
      echo "Exported: $env_var"
    fi
  done < "/workspaces/.remote-env"
fi

# Apply user configuration
if [ -f "/workspaces/.user-config" ]; then
  echo "Applying user configuration from devcontainer.json"
  source /workspaces/.user-config
  
  # Note: Full user switching would require more complex handling
  # This is just setting environment variables for reference
  if [ ! -z "$REMOTE_USER" ]; then
    echo "Remote user set to: $REMOTE_USER"
  fi
  
  if [ ! -z "$CONTAINER_USER" ]; then
    echo "Container user set to: $CONTAINER_USER"
  fi
fi
EOL
            chmod +x /workspaces/setup-env.sh
            
            # Create a script to run lifecycle commands
            echo "Creating lifecycle script"
            cat > /workspaces/run-lifecycle.sh << 'EOL'
#!/bin/bash
cd /workspaces

# # Run start-docker-compose command if it exists
# if [ -f "/workspaces/start-docker-compose.sh" ]; then
#   echo "Running docker-compose..."
#   /workspaces/start-docker-compose.sh
# fi

# # Run post-create command if it exists
# if [ -f "/workspaces/post-create-command.sh" ]; then
#   echo "Running postCreateCommand..."
#   /workspaces/post-create-command.sh
# fi

# Run post-start command if it exists
# if [ -f "/workspaces/post-start-command.sh" ]; then
#   echo "Running postStartCommand..."
#   /workspaces/post-start-command.sh
# fi

# Set up VS Code settings if they exist
if [ -f "/workspaces/.vscode/settings.json" ]; then
  echo "Applying VS Code settings..."
  mkdir -p /config/data/User
  cp /workspaces/.vscode/settings.json /config/data/User/settings.json
fi

# Handle port forwarding if configured
if [ -f "/workspaces/.forward-ports" ]; then
  echo "Setting up port forwarding..."
  # Note: The actual port forwarding is handled by the Kubernetes ingress
  # This is just for informational purposes
  cat /workspaces/.forward-ports
fi
EOL
            chmod +x /workspaces/run-lifecycle.sh
    """
    
    # Add the feature installation script directly
    feature_script = _create_feature_installation_script()
    init_script += """
            # Create features installation script
            echo "Creating features installation script"
            cat > /workspaces/install-features.sh << 'EOL'
"""
    init_script += feature_script
    init_script += """
EOL
            chmod +x /workspaces/install-features.sh
    """
    
    # Continue with the rest of the script
    init_script += """
        fi
        
        # Check if there's a docker-compose.yml file
        if [ -f "$USER_REPO_PATH/.devcontainer/docker-compose.yml" ]; then
            echo "Found docker-compose.yml - copying to build context"
            cp "$USER_REPO_PATH/.devcontainer/docker-compose.yml" /workspaces/.user-dockerfile/
        fi
        
        # Copy any other files in the .devcontainer directory
        if [ -d "$USER_REPO_PATH/.devcontainer" ]; then
            echo "Copying all files from .devcontainer directory"
            cp -r "$USER_REPO_PATH/.devcontainer/"* /workspaces/.user-dockerfile/
        fi
    else
        echo "Warning: No Dockerfile found at $DOCKERFILE_PATH"
        echo "Using default Go dev container image instead"
        echo "FROM mcr.microsoft.com/devcontainers/go:latest" > /workspaces/.user-dockerfile/Dockerfile
    fi
    """
    
    # Add Dockerfile creation
    init_script += f"""
    # Create a wrapper Dockerfile that uses the user's image as a base
    cat > Dockerfile << 'EOF'
# This will be replaced with the tag for the user's custom image
FROM {AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/workspace-images:custom-user-{workspace_ids['namespace_name']}-{workspace_ids['build_timestamp']}

# Install code-server
RUN curl -fsSL https://code-server.dev/install.sh | sh

# Expose default code-server port
EXPOSE 8444

# Set up entrypoint to run code-server
ENTRYPOINT ["/bin/bash", "-c", "if [ -f /workspaces/install-features.sh ]; then /workspaces/install-features.sh; fi && if [ -f /workspaces/setup-env.sh ]; then source /workspaces/setup-env.sh; fi && if [ -f /workspaces/install-extensions.sh ]; then /workspaces/install-extensions.sh; fi && if [ -f /workspaces/run-lifecycle.sh ]; then /workspaces/run-lifecycle.sh & fi && /usr/bin/code-server --bind-addr 0.0.0.0:8444 --auth password --user-data-dir /config/data --extensions-dir /config/extensions /workspaces"]
EOF
    
    # Create a flag file to indicate setup is done
    touch /workspaces/.code-server-initialized
    
    # Initialize workspace
    echo "Workspace initialization completed!"
    """
    
    init_config_map = client.V1ConfigMap(
        metadata=client.V1ObjectMeta(
            name="workspace-init",
            namespace=workspace_ids['namespace_name'],
            labels={"app": "workspace"}
        ),
        data={
            "init.sh": init_script
        }
    )
    core_v1.create_namespaced_config_map(workspace_ids['namespace_name'], init_config_map)
    logger.info(f"Created init script ConfigMap in namespace: {workspace_ids['namespace_name']}")

def _generate_init_script(workspace_ids, workspace_config):
    """Generate the initialization bash script"""
    # Start with base script
    init_script = """#!/bin/bash
      set -e
      set -x

      git config --global --add safe.directory '*'
      
      # Ensure the workspace directory exists
      mkdir -p /workspaces

      # Change to the workspace directory
      cd /workspaces

      # Configure git to use GITHUB_TOKEN for private repos
      if [ ! -z "$GITHUB_TOKEN" ]; then
        echo "Using GITHUB_TOKEN for private repo access"
        git config --global url."https://$GITHUB_TOKEN@github.com/".insteadOf "https://github.com/"
      fi
    """

    repo_names = []

    # Add repository clone commands
    for i, repo_url in enumerate(workspace_config['github_urls']):
        # Extract repo name from the URL
        repo_name_parts = repo_url.rstrip('/').split('/')
        folder_name = repo_name_parts[-1].replace('.git', '') if len(repo_name_parts) > 1 else f"repo-{i}"
        owner = repo_name_parts[-2] if len(repo_name_parts) > 2 else "unknown-owner"

        repo_names.append(folder_name)

        branch = workspace_config['github_branches'][i] if i < len(workspace_config['github_branches']) else ""
            
        if branch:
            init_script += f"""
        # Clone repository {i+1}: {repo_url} (branch: {branch})
        if [ ! -d "/workspaces/{folder_name}" ]; then
            echo "Cloning {repo_url} branch {branch} into {folder_name}..."
            git clone -b {branch} {repo_url} {folder_name}
        fi

        # Mark repo as safe
        git config --global --add safe.directory "/workspaces/{folder_name}"
        """
        else:
            init_script += f"""
        # Clone repository {i+1}: {repo_url} (default branch)
        if [ ! -d "/workspaces/{folder_name}" ]; then
            echo "Cloning {repo_url} into {folder_name}..."
            git clone {repo_url} {folder_name}
        fi

        # Mark repo as safe
        git config --global --add safe.directory "/workspaces/{folder_name}"
        """

        # 🔐 Set the remote URL with GITHUB_TOKEN
        init_script += f"""
        # Set Git remote URL to use GITHUB_TOKEN
        if [ ! -z "$GITHUB_TOKEN" ]; then
            cd /workspaces/{folder_name}
            git remote set-url origin https://$GITHUB_TOKEN@github.com/{owner}/{folder_name}.git
            cd ..
        fi
        """

    # Add custom image building section if required
    if workspace_config['use_custom_image_url']:
        init_script += _generate_custom_image_script(workspace_ids, workspace_config)

    # Add standard initialization code
    init_script += _generate_standard_init_code(repo_names)
    
    return init_script


def _generate_custom_image_script(workspace_ids, workspace_config):
    """Generate script for custom image handling"""
    return f"""
        # Create directory for custom image
        mkdir -p /workspaces/.custom-image
        cd /workspaces/.custom-image
        
        # Download custom image configuration
        echo "Downloading custom image configuration from {workspace_config['custom_image_url']}..."
        if [[ "{workspace_config['custom_image_url']}" == *github* ]]; then
            # If it's a GitHub URL, use special handling
            if [[ "{workspace_config['custom_image_url']}" == *.git ]]; then
                # It's a Git repository
                git clone {workspace_config['custom_image_url']} .
            else:
                # It might be a direct file or directory URL
                # Convert github.com URLs to raw.githubusercontent.com if needed
                RAW_URL=$(echo "{workspace_config['custom_image_url']}" | sed 's|github.com|raw.githubusercontent.com|g' | sed 's|/blob/|/|g')
                curl -L "$RAW_URL" -o dockerfile.zip
                unzip dockerfile.zip
                rm dockerfile.zip
            fi
        else:
            # Regular URL to a file
            curl -L "{workspace_config['custom_image_url']}" -o image-config.zip
            unzip image-config.zip
            rm image-config.zip
        fi
        
        # Check if there's a Dockerfile
        if [ ! -f "Dockerfile" ]; then
            echo "Error: No Dockerfile found in the downloaded configuration"
            echo "Using default image instead: linuxserver/code-server:latest"
            touch /workspaces/.use-default-image
        fi
    """


def _generate_standard_init_code(repo_names):
    """Generate standard initialization code common to all workspaces"""
    script = ""
    for repo_name in repo_names:
        script += f"""
git config --global --add safe.directory /workspaces/{repo_name}
"""

    script += """
# Set up git config if needed
git config --global --add safe.directory /workspaces

git config --global user.email "user@example.com"
git config --global user.name "Code Server User"

# Create Docker helper scripts for the user
cat > /workspaces/docker-info.sh << 'EOF'
#!/bin/bash
echo "Docker is available as a separate daemon inside this container."
echo "The Docker daemon starts automatically and is ready to use."
echo "You can verify it's working by running: docker info"
EOF
chmod +x /workspaces/docker-info.sh

# Create a custom .bashrc extension with Docker information
cat > /workspaces/.bash_docker << 'EOF'
#!/bin/bash

# Display Docker status on login
if command -v docker &> /dev/null; then
    if docker info &>/dev/null; then
        echo "🐳 Docker daemon is running and ready to use!"
        echo "Try running 'docker run hello-world' to test it."
    else
        echo "⚠️ Docker CLI is installed but the daemon isn't responding."
        echo "The daemon may still be starting up. Try again in a moment."
    fi
else
    echo "⚠️ Docker CLI is not installed. Something went wrong with the setup."
fi

# Add Docker-related aliases
alias d='docker'
alias dc='docker-compose'
alias dps='docker ps'
alias di='docker images'
EOF

      touch /workspaces/.code-server-initialized

      # Initialize workspace
      echo "Workspace initialized successfully!"
  """
    
    return script


def _create_workspace_info_configmap(workspace_ids, workspace_config):
    """Create ConfigMap with workspace information"""
    workspace_info = _get_workspace_info(workspace_ids, workspace_config)
    
    info_config_map = client.V1ConfigMap(
        metadata=client.V1ObjectMeta(
            name="workspace-info",
            namespace=workspace_ids['namespace_name'],
            labels={"app": "workspace-info"}
        ),
        data={
            "info": json.dumps(workspace_info)
        }
    )
    core_v1.create_namespaced_config_map(workspace_ids['namespace_name'], info_config_map)
    logger.info(f"Created workspace info ConfigMap in namespace: {workspace_ids['namespace_name']}")


def _get_workspace_info(workspace_ids, workspace_config):
    """Create the workspace information dictionary"""
    workspace_info = {
        "id": workspace_ids['workspace_id'],
        "repositories": workspace_config['github_urls'],
        "branches": workspace_config['github_branches'],
        "primaryRepo": workspace_config['primary_repo_url'],
        "repoName": workspace_config['repo_name'],
        "subdomain": workspace_ids['subdomain'],
        "fqdn": workspace_ids['fqdn'],
        "url": f"https://{workspace_ids['fqdn']}",
        "password": workspace_ids['password'],
        "created": datetime.now().isoformat()
    }

    if workspace_config['use_custom_image_url']:
        workspace_info["imageUrl"] = workspace_config['custom_image_url']
        workspace_info["customImage"] = True
    else:
        workspace_info["image"] = workspace_config['custom_image']
        workspace_info["customImage"] = False
        workspace_info["useDevContainer"] = workspace_config['use_dev_container']
        
    return workspace_info


def _copy_port_detector_configmap(workspace_ids):
    """Copy port-detector ConfigMap from workspace-system to the new namespace"""
    try:
        # Get the ConfigMap from workspace-system
        port_detector_cm = core_v1.read_namespaced_config_map(
            name="port-detector", 
            namespace="workspace-system"
        )
        
        # Create a new ConfigMap in the workspace namespace
        new_cm = client.V1ConfigMap(
            metadata=client.V1ObjectMeta(
                name="port-detector",
                namespace=workspace_ids['namespace_name'],
                labels={"app": "workspace"}
            ),
            data=port_detector_cm.data  # Copy the data from the original ConfigMap
        )
        
        # Create the ConfigMap in the new namespace
        core_v1.create_namespaced_config_map(workspace_ids['namespace_name'], new_cm)
        logger.info(f"Copied port-detector ConfigMap to namespace: {workspace_ids['namespace_name']}")
        
    except Exception as e:
        logger.error(f"Error copying port-detector ConfigMap: {e}")
        # Continue anyway, as this is not critical


def _copy_wildcard_certificate(workspace_ids):
    """Copy wildcard certificate from workspace-system to the new namespace"""
    try:
        # Check if the wildcard certificate secret exists in workspace-system
        wildcard_cert = core_v1.read_namespaced_secret(
            name="workspace-domain-wildcard-tls", 
            namespace="workspace-system"
        )
        
        # Create a new secret in the workspace namespace with the same data
        wildcard_cert_data = wildcard_cert.data
        wildcard_cert_new = client.V1Secret(
            metadata=client.V1ObjectMeta(
                name="workspace-domain-wildcard-tls",
                namespace=workspace_ids['namespace_name'],
                labels={"app": "workspace"}
            ),
            data=wildcard_cert_data,
            type=wildcard_cert.type
        )
        
        # Create the secret in the new namespace
        core_v1.create_namespaced_secret(workspace_ids['namespace_name'], wildcard_cert_new)
        logger.info(f"Copied wildcard certificate secret to namespace: {workspace_ids['namespace_name']}")
        
    except Exception as e:
        logger.error(f"Error copying wildcard certificate: {e}")
        # Continue anyway, but log it - this might cause SSL errors


def _create_deployment(workspace_ids, workspace_config):
    """Create deployment for the code-server"""
    # Create storage for local registry
    _create_pvc_for_registry(workspace_ids)

    # Define init containers
    init_containers = _create_init_containers(workspace_ids, workspace_config)

    # Define volumes
    volumes = _create_volumes(workspace_ids)

    # try:
    #     # Get the CA certificate from the workspace-system namespace
    #     registry_ca = client.CoreV1Api().read_namespaced_config_map(
    #         name="registry-ca",
    #         namespace="workspace-system"
    #     )
        
    #     # Create the same ConfigMap in the workspace namespace
    #     ca_cm = client.V1ConfigMap(
    #         metadata=client.V1ObjectMeta(
    #             name="registry-ca",
    #             namespace=workspace_ids['namespace_name']
    #         ),
    #         data=registry_ca.data
    #     )
        
    #     client.CoreV1Api().create_namespaced_config_map(
    #         namespace=workspace_ids['namespace_name'],
    #         body=ca_cm
    #     )
        
    #     logging.info(f"Copied registry CA to namespace {workspace_ids['namespace_name']}")
    # except Exception as e:
    #     logging.error(f"Error copying registry CA: {e}")

    
    # Create a Docker config JSON with registry authentication
    auth_config = {
        "auths": {
            "registry.workspace-system.svc.cluster.local:5000": {
                "auth": ""  # Empty auth for registry without username/password
            }
        }
    }

    # Convert to base64
    auth_json = json.dumps(auth_config).encode()
    auth_b64 = base64.b64encode(auth_json).decode()

    # Create the secret
    registry_secret = client.V1Secret(
        metadata=client.V1ObjectMeta(
            name="registry-credentials",
            namespace=workspace_ids['namespace_name']
        ),
        type="kubernetes.io/dockerconfigjson",
        data={
            ".dockerconfigjson": auth_b64
        }
    )

    # Create the secret in the namespace
    client.CoreV1Api().create_namespaced_secret(
        namespace=workspace_ids['namespace_name'],
        body=registry_secret
    )

    create_service_workspace_account(workspace_ids['namespace_name'])

    # Define containers
    code_server_container = _create_code_server_container(workspace_ids, workspace_config)
    port_detector_container = _create_port_detector_container()

    deployment = client.V1Deployment(
        metadata=client.V1ObjectMeta(
            name="code-server",
            namespace=workspace_ids['namespace_name'],
            labels={
                "app": "workspace",
                "allowed-registry-access": "true"
            }
        ),
        spec=client.V1DeploymentSpec(
            replicas=1,
            selector=client.V1LabelSelector(
                match_labels={"app": "code-server"}
            ),
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(
                    labels={
                        "app": "code-server",
                        "allowed-registry-access": "true"
                    },
                    annotations={
                        # Add this to allow insecure registry
                        "container.apparmor.security.beta.kubernetes.io/code-server": "unconfined",
                        "deployment.kubernetes.io/revision": str(int(time.time())),
                        "kubectl.kubernetes.io/restartedAt": str(int(time.time()))
                    }
                ),
                spec=client.V1PodSpec(
                    host_network=True,
                    service_account_name="workspace-controller",
                    init_containers=init_containers,
                    containers=[code_server_container, port_detector_container],
                    volumes=volumes,
                    image_pull_secrets=[
                        client.V1LocalObjectReference(name="registry-credentials")
                    ]
                )
            )
        )
    )

    apps_v1.create_namespaced_deployment(workspace_ids['namespace_name'], deployment)
    logger.info(f"Created deployment in namespace: {workspace_ids['namespace_name']}")


def _create_init_containers(workspace_ids, workspace_config):
    """Create the initialization containers for the deployment"""
    init_containers = [
        _create_workspace_init_container()
    ]

    # init_containers.append(
    #     client.V1Container(
    #         name="update-ca-certificates",
    #         image="ubuntu:20.04",
    #         command=["/bin/sh", "-c"],
    #         args=[
    #             "apt-get update && apt-get install -y ca-certificates && " +
    #             "cp /registry-ca/ca.crt /usr/local/share/ca-certificates/ && " +
    #             "update-ca-certificates && " +
    #             "echo 'CA certificates updated'"
    #         ],
    #         volume_mounts=[
    #             client.V1VolumeMount(
    #                 name="registry-ca",
    #                 mount_path="/registry-ca"
    #             )
    #         ]
    #     )
    # )
    
    # Add code-server setup container when using dev container mode
    # if workspace_config['use_dev_container']:
    #     init_containers.append(_create_codeserver_setup_container())
    
    # init_containers.append(_create_custom_image_build_container(workspace_ids))

    # Add build container for custom image if URL is provided
    # if workspace_config['use_custom_image_url']:
    #     init_containers.append(_create_custom_image_build_container(workspace_ids))
    
    init_containers.append(_create_base_image_kaniko_container(workspace_ids)),
    init_containers.append(_create_wrapper_kaniko_container(workspace_ids))

    return init_containers


def _create_workspace_init_container():
    """Create the main workspace initialization container"""
    env_vars = []
    # Add GITHUB_TOKEN env var if present in secret
    env_vars.append(
        client.V1EnvVar(
            name="GITHUB_TOKEN",
            value_from=client.V1EnvVarSource(
                secret_key_ref=client.V1SecretKeySelector(
                    name="workspace-secret",
                    key="github_token",
                    optional=True
                )
            )
        )
    )
    return client.V1Container(
        name="init-workspace",
        image="buildpack-deps:22.04-scm",
        command=["/bin/bash", "/scripts/init.sh"],
        security_context=client.V1SecurityContext(
            capabilities=client.V1Capabilities(
                add=["CHOWN", "FOWNER", "FSETID", "DAC_OVERRIDE"]
            )
        ),
        volume_mounts=[
            client.V1VolumeMount(
                name="workspace-data",
                mount_path="/config",  # LinuxServer.io's main config directory
                sub_path="config"
            ),
            client.V1VolumeMount(
                name="workspace-data",
                mount_path="/workspaces",
                sub_path="workspaces"
            ),
            client.V1VolumeMount(
                name="init-script",  # Add this new volume mount
                mount_path="/scripts"  # Match the command's expected path
            ),
            client.V1VolumeMount(
                name="docker-sock",
                mount_path="/var/run/docker.sock"
            )
        ],
        env=env_vars
    )

def _create_pvc_for_registry(workspace_ids):
    """Create PVC for local registry storage"""
    pvc = client.V1PersistentVolumeClaim(
        metadata=client.V1ObjectMeta(
            name="registry-storage",
            namespace=workspace_ids['namespace_name']
        ),
        spec=client.V1PersistentVolumeClaimSpec(
            access_modes=["ReadWriteOnce"],
            resources=client.V1ResourceRequirements(
                requests={"storage": "5Gi"}
            ),
            storage_class_name="efs-sc"
        )
    )
    core_v1.create_namespaced_persistent_volume_claim(workspace_ids['namespace_name'], pvc)
    logger.info(f"Created registry storage PVC in namespace: {workspace_ids['namespace_name']}")


def _create_base_image_kaniko_container(workspace_ids):
    """Create container for building user's base Docker image using Kaniko"""

    return client.V1Container(
        name="build-base-image",
        image="gcr.io/kaniko-project/executor:latest",
        args=[
            "--dockerfile=/workspace/Dockerfile",
            "--context=/workspace",
            f"--destination={AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/workspace-images:custom-user-{workspace_ids['namespace_name']}-{workspace_ids['build_timestamp']}",
            "--insecure",
            "--skip-tls-verify",
            "--verbosity=debug",
            "--push-retry=3"
        ],
        env=[
            client.V1EnvVar(name="DOCKER_CONFIG", value="/kaniko/.docker/"),
            client.V1EnvVar(name="HTTP_TIMEOUT", value="600s"),  # Increase timeout
            client.V1EnvVar(name="HTTPS_TIMEOUT", value="600s")
        ],
        volume_mounts=[
            client.V1VolumeMount(
                name="workspace-data",
                mount_path="/workspace",
                sub_path="workspaces/.user-dockerfile"  # Path to the user's Dockerfile
            )
        ]
    )


def _create_wrapper_kaniko_container(workspace_ids):
    """Create container for building code-server wrapper image using Kaniko"""
    nodes = client.CoreV1Api().list_node()
    node_ip = nodes.items[0].status.addresses[0].address

    return client.V1Container(
        name="build-wrapper-image",
        image="gcr.io/kaniko-project/executor:latest",
        args=[
            "--dockerfile=/workspace/Dockerfile",
            "--context=/workspace",
            f"--destination={AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/workspace-images:custom-wrapper-{workspace_ids['namespace_name']}-{workspace_ids['build_timestamp']}",
            "--insecure",
            "--skip-tls-verify",
            "--verbosity=debug",
            "--push-retry=3"
        ],
        env=[
            client.V1EnvVar(name="DOCKER_CONFIG", value="/kaniko/.docker/"),
            client.V1EnvVar(name="HTTP_TIMEOUT", value="600s"),  # Increase timeout
            client.V1EnvVar(name="HTTPS_TIMEOUT", value="600s")
        ],
        volume_mounts=[
            client.V1VolumeMount(
                name="workspace-data",
                mount_path="/workspace",
                sub_path="workspaces/.code-server-wrapper"
            )
        ]
    )

def _create_volumes(workspace_ids):
    """Create the volume definitions for the deployment"""
    return [
        client.V1Volume(
            name="workspace-data",
            persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(
                claim_name="workspace-data"
            )
        ),
        client.V1Volume(
            name="registry-storage",
            persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(
                claim_name="registry-storage"
            )
        ),
        client.V1Volume(
            name="init-script",
            config_map=client.V1ConfigMapVolumeSource(
                name="workspace-init",
                default_mode=0o755
            )
        ),
        # Add volume for code-server in dev container mode
        client.V1Volume(
            name="code-server-data",
            empty_dir=client.V1EmptyDirVolumeSource()
        ),
        # Docker volumes
        client.V1Volume(
            name="docker-lib",
            empty_dir=client.V1EmptyDirVolumeSource()
        ),
        client.V1Volume(
            name="docker-sock",
            empty_dir=client.V1EmptyDirVolumeSource()
        ),
        # Port detector script
        client.V1Volume(
            name="port-detector-script",
            config_map=client.V1ConfigMapVolumeSource(
                name="port-detector",
                default_mode=0o755
            )
        # ),
        # client.V1Volume(
        #     name="registry-ca",
        #     config_map=client.V1ConfigMapVolumeSource(
        #         name="registry-ca"
        #     )
        )
    ]

def create_service_workspace_account(workspace_namespace):
    service_account = client.V1ServiceAccount(
        metadata=client.V1ObjectMeta(
            name="workspace-controller",
            namespace=workspace_namespace,
            annotations={
                "eks.amazonaws.com/role-arn": f"arn:aws:iam::{AWS_ACCOUNT_ID}:role/workspace-controller-role"
            }
        )
    )
    
    try:
        api_instance = client.CoreV1Api()
        api_instance.create_namespaced_service_account(
            namespace=workspace_namespace,
            body=service_account
        )
        print(f"Created service account in namespace {workspace_namespace}")
    except Exception as e:
        print(f"Error creating service account: {e}")

def _create_code_server_container(workspace_ids, workspace_config):
    """Create the main code-server container"""
    # Set the container image based on configuration
    # container_image = workspace_config['custom_image']
    image_pull_policy = "Always"

    return client.V1Container(
        name="code-server",
        image=f"{AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/workspace-images:custom-wrapper-{workspace_ids['namespace_name']}-{workspace_ids['build_timestamp']}",
        image_pull_policy=image_pull_policy,
        # ports=[
        #     client.V1ContainerPort(container_port=8444)
        # ],
        env=[
            # LinuxServer.io specific environment variables
            client.V1EnvVar(name="PUID", value="1000"),  # User ID
            client.V1EnvVar(name="PGID", value="1000"),  # Group ID
            client.V1EnvVar(name="TZ", value="UTC"),  # Timezone
            client.V1EnvVar(name="DEFAULT_WORKSPACE", value="/workspaces"),  
            client.V1EnvVar(name="VSCODE_EXTENSIONS", value="/config/extensions"),
            client.V1EnvVar(name="CODE_SERVER_EXTENSIONS_DIR", value="/config/extensions"),
            client.V1EnvVar(name="VSCODE_USER_DATA_DIR", value="/config/data"),
            client.V1EnvVar(name="CS_DISABLE_GETTING_STARTED_OVERRIDE", value="true"),
            client.V1EnvVar(name="VSCODE_PROXY_URI", value=f"https://{workspace_ids['subdomain']}-{{{{port}}}}.{WORKSPACE_DOMAIN}/"),
            # Authentication and core settings
            client.V1EnvVar(
                name="PASSWORD",
                value_from=client.V1EnvVarSource(
                    secret_key_ref=client.V1SecretKeySelector(
                        name="workspace-secret",
                        key="password"
                    )
                )
            ),
            # Docker support
            client.V1EnvVar(name="DOCKER_HOST", value="unix:///var/run/docker.sock"),
            # Add this for dev container mode
            client.V1EnvVar(name="CODE_SERVER_PATH", value="/opt/code-server/bin/code-server" if workspace_config['use_dev_container'] else ""),
        ],
        volume_mounts=_create_code_server_volume_mounts(workspace_config),
        lifecycle=client.V1Lifecycle(
            post_start=client.V1LifecycleHandler(
                _exec=client.V1ExecAction(
                    command=_create_post_start_command()
                )
            )
        ),
        security_context=client.V1SecurityContext(
            run_as_user=0 if workspace_config['use_dev_container'] else None,
            privileged=True,  # Ensure full access to Docker
            capabilities=client.V1Capabilities(
                add=["SYS_ADMIN", "NET_ADMIN"]
            )
        ),
        resources=client.V1ResourceRequirements(
            requests={
                "cpu": "4",
                "memory": "8Gi"
            },
            limits={
                "cpu": "4",
                "memory": "8Gi"
            }
        )
    )

def _create_code_server_volume_mounts(workspace_config):
    """Create volume mounts for the code-server container"""
    volume_mounts = [
        client.V1VolumeMount(
            name="workspace-data",
            mount_path="/config",  # LinuxServer.io uses /config for persistent data
            sub_path="config"
        ),
        client.V1VolumeMount(
            name="workspace-data",
            mount_path="/workspaces",
            sub_path="workspaces"
        ),
        # Docker daemon storage and socket
        client.V1VolumeMount(
            name="docker-lib",
            mount_path="/var/lib/docker"
        ),
        client.V1VolumeMount(
            name="docker-sock",
            mount_path="/var/run"
        # ),
        # client.V1VolumeMount(
        #     name="registry-ca",
        #     mount_path="/usr/local/share/ca-certificates/registry-ca.crt",
        #     sub_path="ca.crt"
        )
    ]
    
    # Add the code-server volume mount when in dev container mode
    if workspace_config['use_dev_container']:
        volume_mounts.append(
            client.V1VolumeMount(
                name="code-server-data",
                mount_path="/opt/code-server"
            )
        )
    
    return volume_mounts

@app.route('/api/workspaces/<workspace_id>', methods=['GET'])
@token_required
def get_workspace(current_user, workspace_id):
    """Get details for a specific workspace"""
    try:
        # Find the namespace for this workspace
        namespaces = core_v1.list_namespace(label_selector=f"workspaceId={workspace_id}")
        
        if not namespaces.items:
            return jsonify({"error": "Workspace not found"}), 404
            
        namespace_name = namespaces.items[0].metadata.name
        
        # Get workspace info from config map
        config_maps = core_v1.list_namespaced_config_map(namespace_name, label_selector="app=workspace-info")
        if not config_maps.items:
            return jsonify({"error": "Workspace info not found"}), 404
            
        workspace_info = json.loads(config_maps.items[0].data.get("info", "{}"))
        
        # Don't expose password unless explicitly requested
        if "password" in workspace_info and request.args.get("includePassword") != "true":
            workspace_info["password"] = "********"
        
        # Get pods to determine state
        pods = core_v1.list_namespaced_pod(namespace_name, label_selector="app=code-server")
        if pods.items:
            if pods.items[0].status.phase == "Running":
                workspace_info["state"] = "running"
            else:
                workspace_info["state"] = pods.items[0].status.phase.lower()
        else:
            workspace_info["state"] = "unknown"
        
        return jsonify(workspace_info)
    except Exception as e:
        logger.error(f"Error getting workspace: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/workspaces/<workspace_id>/delete', methods=['DELETE'])
@token_required
def delete_workspace(current_user, workspace_id):
    """Delete a workspace"""
    try:
        # Find the namespace for this workspace
        namespaces = core_v1.list_namespace(label_selector=f"workspaceId={workspace_id}")
        
        if not namespaces.items:
            return jsonify({"error": "Workspace not found"}), 404
            
        namespace_name = namespaces.items[0].metadata.name
        
        # Delete the namespace (this will delete all resources in it)
        core_v1.delete_namespace(namespace_name)
        
        return jsonify({
            "success": True,
            "message": f"Workspace {workspace_id} deleted"
        })
    except Exception as e:
        logger.error(f"Error deleting workspace: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/workspaces/<workspace_id>/stop', methods=['POST'])
@token_required
def stop_workspace(current_user, workspace_id):
    """Stop a workspace by scaling it to 0 replicas"""
    try:
        # Find the namespace for this workspace
        namespaces = core_v1.list_namespace(label_selector=f"workspaceId={workspace_id}")
        
        if not namespaces.items:
            return jsonify({"error": "Workspace not found"}), 404
            
        namespace_name = namespaces.items[0].metadata.name
        
        # Scale the deployment to 0
        apps_v1.patch_namespaced_deployment_scale(
            name="code-server",
            namespace=namespace_name,
            body={"spec": {"replicas": 0}}
        )
        
        return jsonify({
            "success": True,
            "message": f"Workspace {workspace_id} stopped"
        })
    except Exception as e:
        logger.error(f"Error stopping workspace: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/workspaces/<workspace_id>/start', methods=['POST'])
@token_required
def start_workspace(current_user, workspace_id):
    """Start a workspace by scaling it to 1 replica"""
    try:
        # Find the namespace for this workspace
        namespaces = core_v1.list_namespace(label_selector=f"workspaceId={workspace_id}")
        
        if not namespaces.items:
            return jsonify({"error": "Workspace not found"}), 404
            
        namespace_name = namespaces.items[0].metadata.name
        
        # Scale the deployment to 1
        apps_v1.patch_namespaced_deployment_scale(
            name="code-server",
            namespace=namespace_name,
            body={"spec": {"replicas": 1}}
        )
        
        return jsonify({
            "success": True,
            "message": f"Workspace {workspace_id} started"
        })
    except Exception as e:
        logger.error(f"Error starting workspace: {e}")
        return jsonify({"error": str(e)}), 500

# Authentication endpoints
@app.route('/api/auth/login', methods=['POST'])
def login():
    """Authenticate user and return JWT token"""
    try:
        data = request.get_json()
        username = data.get('username')
        password = data.get('password')
        
        if not username or not password:
            return jsonify({
                'success': False,
                'error': 'Username and password are required'
            }), 400
        
        # Find user in configuration
        user = None
        for u in USERS_CONFIG.get('users', []):
            if u['username'] == username:
                user = u
                break
        
        if not user:
            return jsonify({
                'success': False,
                'error': 'Invalid username or password'
            }), 401
        
        # Verify password
        if not bcrypt.checkpw(password.encode('utf-8'), user['password'].encode('utf-8')):
            return jsonify({
                'success': False,
                'error': 'Invalid username or password'
            }), 401
        
        # Generate JWT token
        payload = {
            'username': user['username'],
            'role': user.get('role', 'user'),
            'iat': datetime.now(timezone.utc),
            'exp': datetime.now(timezone.utc) + timedelta(hours=24)  # Token expires in 24 hours
        }
        
        token = jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256')
        
        return jsonify({
            'success': True,
            'token': token,
            'user': {
                'username': user['username'],
                'role': user.get('role', 'user')
            }
        })
        
    except Exception as e:
        logger.error(f"Login error: {e}")
        return jsonify({
            'success': False,
            'error': 'Authentication failed'
        }), 500

@app.route('/api/auth/verify', methods=['GET'])
@token_required
def verify_token(current_user):
    """Verify if the current token is valid"""
    return jsonify({
        'success': True,
        'user': {
            'username': current_user['username'],
            'role': current_user['role']
        }
    })

@app.route('/api/auth/refresh', methods=['POST'])
@token_required
def refresh_token(current_user):
    """Refresh the JWT token"""
    try:
        # Generate new JWT token
        payload = {
            'username': current_user['username'],
            'role': current_user['role'],
            'iat': datetime.now(timezone.utc),
            'exp': datetime.now(timezone.utc) + timedelta(hours=24)
        }
        
        token = jwt.encode(payload, JWT_SECRET_KEY, algorithm='HS256')
        
        return jsonify({
            'success': True,
            'token': token,
            'user': {
                'username': current_user['username'],
                'role': current_user['role']
            }
        })
        
    except Exception as e:
        logger.error(f"Token refresh error: {e}")
        return jsonify({
            'success': False,
            'error': 'Token refresh failed'
        }), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=3000)